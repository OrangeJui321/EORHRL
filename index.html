<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/EORHRL/img/favicon.ico">
    <title>eorhrl</title>
    
<link rel="stylesheet" href="/EORHRL/css/reset.css">

    
<link rel="stylesheet" href="/EORHRL/css/style.css">

    
<link rel="stylesheet" href="/EORHRL/css/markdown.css">

    
<link rel="stylesheet" href="/EORHRL/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 7.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/EORHRL/2025/02/26/paper/">Supportive Relationships-aware Hierarchical Reinforcement Learning for Efficient Ex-situ Object Rearrangement</a> -->
        <div class="post-title" >Supportive Relationships-aware Hierarchical Reinforcement Learning for Efficient Ex-situ Object Rearrangement</div>
        <div class="post-authors" >Leibing Xiao¹, Jin Liu¹, Yachao Wang¹, Xuemei Wang¹,Kuanqi Cai², Chaoqun Wang¹</div>
        <div class="post-affiliations" >¹Shandong University,²Italian Institute of Technology</div>
        <div class="head-icon"> 

            

            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >In object rearrangement tasks within open environments, robots face significant challenges due to the increased cost of moving objects over large workspaces. As the workspace expands, the time and energy required to move objects from their initial positions to target locations increase, which can severely impact the efficiency of the task. To address this issue, we propose a hierarchical reinforcement learning-based approach that takes into account the supportive relationships and semantic correlations between objects. The robot groups and stacks objects with compatible supportive capabilities, moving them together to their target locations to optimize task execution. Specifically, the robot uses a large language model to assess the supportive relationships and semantic correlations between objects. In the high-level decision-making process, objects are grouped based on their stacking capabilities, while the low-level process refines these groupings using a graph capsule convolutional network. Experimental results demonstrate that our approach not only reduces the number of movements required but also improves task efficiency and significantly decreases task completion time—by approximately 50\%—compared to methods that do not consider supportive relationships. Further validation in real-world scenarios confirms the effectiveness of our method.</div>
            
        </div>
        
        <div class="post-except">
            <!-- ## The *listen-perceive-grasp* paradigm for robotic grasp reasoning
The listen-perceive-grasp paradigm for robotic grasp reasoning. Given a natural language description of a desired object in the scene, a human is able to adeptly listen for the gist of the description and understand linguistic and visual concepts, then perceive and delimit the referred object, and ultimately grasp the target by factoring in the physical attribute such as shape, scale, and spatial location. This process is similarly applicable to robots. The visual attributes serve to constrain the both width and position of the grasping rectangle, thereby preventing collisions with other objects in the clutter.
![listen-perceive-grasp paradigm](./images/Schematic.jpg) -->
<h2 id="APPROACH"><a href="#APPROACH" class="headerlink" title="APPROACH"></a>APPROACH</h2><p>First, the object information is fed into a large model structure to determine the supportive relationships and semantic correlations between the objects. Next, the high-level reinforcement learning decision process groups the target objects based on their supportive capabilities. In the lower-level reinforcement learning decision process, the robot receives the high-level decision results along with the semantic and supportive relationship information, which are then used to finalize the object grouping. Finally, the robot executes the grouping decision.<br><img src="/EORHRL/./images/frame.png" alt="The hierarchical reinforcement learning framework for multi-object rearrangement.The objects are scattered on a white tabletop, and the goal is to move them to a distant wooden cabinet and arrange them neatly. The robot first uses a large language model to determine the supportive relationships and semantic correlations between the objects. In the high-level reinforcement learning decision process, the robot groups the objects based on their supportive capabilities. The low-level decision process then refines the grouping, using semantic and supportive information, before executing the final result."></p>
<h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Simulation-environment"><a href="#Simulation-environment" class="headerlink" title="Simulation environment"></a>Simulation environment</h3><p><img src="/EORHRL/./images/md1.png" alt="Simulation environment"></p>
<h3 id="Simulation-results"><a href="#Simulation-results" class="headerlink" title="Simulation results"></a>Simulation results</h3><p>we first experiment with the object supportive capability grouping module. Comparison of object grouping based on supportive capability using different methods.<br><img src="/EORHRL/./images/md2.png" alt="Comparison of object grouping based on supportive capability using different methods."><br>Then, we compared the graph capsule convolutional neural network method used in this paper with the network framework from AM-RL. The training results of the stacking planning module. (a)-(c) show the results when the number of objects is 30, (d)-(f) show the results when the number of objects is 40, and (g)-(i) show the results when the number of objects is 50.<br><img src="/EORHRL/./images/md3.png" alt="The training results of the stacking planning module. (a)-(c) show the results when the number of objects is 30, (d)-(f) show the results when the number of objects is 40, and (g)-(i) show the results when the number of objects is 50."></p>
<h3 id="Experiments-on-Physical-Platform"><a href="#Experiments-on-Physical-Platform" class="headerlink" title="Experiments on Physical Platform"></a>Experiments on Physical Platform</h3><p>To evaluate the effectiveness of the proposed method in a real-world physical environment, we conducted experiments on an actual physical platform. We used a mobile platform equipped with a Franka Emika Panda robotic arm and a Realsense D435 camera.<br>Comparison of object grouping based on supportive capability using different methods when the number of objects is 50.<br><img src="/EORHRL/./images/real_world3.jpg"><br>The execution process of the robot when rearranging 20 objects with a maximum depth of 5.(a) The initial scene. (b) The result of the object grouping. (c) The result of the stacking planning.</p>
<!-- ![](./images/real_world1.png) -->
<p>The execution process of the robot in the scene.<br><img src="/EORHRL/./images/real_world2.png"></p>

            <!-- <a class="read-more" href="/EORHRL/2025/02/26/paper/"> ... </a> -->
        </div>
        <div class="post-date">2025.02.26</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright © 2025 eorhrl</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Leibing Xiao</a></span>
</div>


<link rel="stylesheet" href="/EORHRL/css/a11y-dark.min.css">


<script src="/EORHRL/js/highlight.min.js"></script>


<script src="/EORHRL/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>