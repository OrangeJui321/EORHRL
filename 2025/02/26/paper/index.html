<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/EORHRL/img/favicon.ico">
    <title>eorhrl</title>
    
<link rel="stylesheet" href="/EORHRL/css/reset.css">

    
<link rel="stylesheet" href="/EORHRL/css/style.css">

    
<link rel="stylesheet" href="/EORHRL/css/markdown.css">

    
<link rel="stylesheet" href="/EORHRL/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 7.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-main">

    
    
        <div class="post-main-title">
            Supportive Relationships-aware Hierarchical Reinforcement Learning for Efficient Ex-situ Object Rearrangement
        </div>
        <div class="post-meta">
            2025-02-26
        </div>
        <div class="post-md">
            <!-- ## The *listen-perceive-grasp* paradigm for robotic grasp reasoning
The listen-perceive-grasp paradigm for robotic grasp reasoning. Given a natural language description of a desired object in the scene, a human is able to adeptly listen for the gist of the description and understand linguistic and visual concepts, then perceive and delimit the referred object, and ultimately grasp the target by factoring in the physical attribute such as shape, scale, and spatial location. This process is similarly applicable to robots. The visual attributes serve to constrain the both width and position of the grasping rectangle, thereby preventing collisions with other objects in the clutter.
![listen-perceive-grasp paradigm](./images/Schematic.jpg) -->
<h2 id="APPROACH"><a href="#APPROACH" class="headerlink" title="APPROACH"></a>APPROACH</h2><p>First, the object information is fed into a large model structure to determine the supportive relationships and semantic correlations between the objects. Next, the high-level reinforcement learning decision process groups the target objects based on their supportive capabilities. In the lower-level reinforcement learning decision process, the robot receives the high-level decision results along with the semantic and supportive relationship information, which are then used to finalize the object grouping. Finally, the robot executes the grouping decision.<br><img src="/EORHRL/./images/architecture.jpg" alt="The hierarchical reinforcement learning framework for multi-object rearrangement.The objects are scattered on a white tabletop, and the goal is to move them to a distant wooden cabinet and arrange them neatly. The robot first uses a large language model to determine the supportive relationships and semantic correlations between the objects. In the high-level reinforcement learning decision process, the robot groups the objects based on their supportive capabilities. The low-level decision process then refines the grouping, using semantic and supportive information, before executing the final result."></p>
<h2 id="EXPERIMENTS"><a href="#EXPERIMENTS" class="headerlink" title="EXPERIMENTS"></a>EXPERIMENTS</h2><h3 id="Simulation-environment"><a href="#Simulation-environment" class="headerlink" title="Simulation environment"></a>Simulation environment</h3><p><img src="/EORHRL/./images/md1.png" alt="Simulation environment"></p>
<h3 id="Simulation-results"><a href="#Simulation-results" class="headerlink" title="Simulation results"></a>Simulation results</h3><p>we first experiment with the object supportive capability grouping module. Comparison of object grouping based on supportive capability using different methods.<br><img src="/EORHRL/./images/md2.png" alt="Comparison of object grouping based on supportive capability using different methods."><br>Then, we compared the graph capsule convolutional neural network method used in this paper with the network framework from AM-RL. The training results of the stacking planning module. (a)-(c) show the results when the number of objects is 30, (d)-(f) show the results when the number of objects is 40, and (g)-(i) show the results when the number of objects is 50.<br><img src="/EORHRL/./images/md3.png" alt="The training results of the stacking planning module. (a)-(c) show the results when the number of objects is 30, (d)-(f) show the results when the number of objects is 40, and (g)-(i) show the results when the number of objects is 50."></p>
<h3 id="Experiments-on-Physical-Platform"><a href="#Experiments-on-Physical-Platform" class="headerlink" title="Experiments on Physical Platform"></a>Experiments on Physical Platform</h3><p>To evaluate the effectiveness of the proposed method in a real-world physical environment, we conducted experiments on an actual physical platform. We used a mobile platform equipped with a Franka Emika Panda robotic arm and a Realsense D435 camera.<br>Comparison of object grouping based on supportive capability using different methods when the number of objects is 50.<br><img src="/EORHRL/./images/real_world3.jpg"></p>
<!-- The execution process of the robot when rearranging 20 objects with a maximum depth of 5.(a) The initial scene. (b) The result of the object grouping. (c) The result of the stacking planning. -->
<!-- ![](./images/real_world1.png) -->
<p>The execution process of the robot in the scene.<br><img src="/EORHRL/./images/real_world2.png"></p>

        </div>

    

</div>
                <div class="footer">
    <span>Copyright Â© 2025 eorhrl</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Leibing Xiao</a></span>
</div>


<link rel="stylesheet" href="/EORHRL/css/a11y-dark.min.css">


<script src="/EORHRL/js/highlight.min.js"></script>


<script src="/EORHRL/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>